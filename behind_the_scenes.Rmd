---
title: "Final Project - Behind The Scenes"
author: "Anael Kuperwajs Cohen, Colleen Minnihan, Hayley Hadges, Thy Nguyen"
date: "5/05/2021"
output: 
  html_document:
    toc: true
    toc_float: true
    df_print: paged
    code_download: true
---

# Introduction

Computer Science is a field that is growing rapidly in the United States and around the world today. Advancements in computer science from industry are constantly being released and technology is becoming more ingrained into our daily lives. The increasing demand of computer scientists had caused the occupation to grow in popularity. To meet the demand, educational institutions and systems are increasing the amount of courses offered in order to train more future computer scientists. This development started at the college level, where majoring in computer science is becoming a widely available option. At Macalester College, it is one of the largest departments for both students and faculty. While the availability of courses at the college level is an amazing start, there is a big push to have computer sciences courses offered in K-12 education. Offering computer science courses in elementary and secondary schools provides an opportunity for kids to expose themselves to coding. This could lead younger students to discover new interests and get engaged with computer science earlier. Often, being exposed to computer science at a younger age can make students more comfortable with the material and the field later on. This can lead to a more empowered and diverse set of students entering the workforce or higher education. Given the importance of having computer science courses available in K-12 education, we decided to explore the availability of computer science courses in K-12 school districts in Minnesota. For our Advanced Data Science final project, we will explore the connection between a variety of datasets related to this topic, including K-12 computer science course availability in Minnesota, demographic information from the U.S. census, ACT scores, and funding.

To see our final product, visit: https://advanced-data-science-final-project.netlify.app

# About the Datasets

**Minnesota Common Course Catalogue**

This data set is provided by the Minnesota State Department of Education and is available to the public. The Minnesota Common Course Catalogue (MCCC) presents information about all the courses that are offered across school districts in Minnesota. For our purposes, we are looking at subject area 10, computer and information sciences (K-12), and all categories and classifications. For each course classification, there is information about the district where it is taught, the region of the state, the local course title, whether it is an AP or IB course, if it fulfills a graduation requirement, among others. We joined this dataset with information about what category the course classification falls into, such as A - computer literacy, B - management information systems, or C - network systems. The final dataset has 1,602 computer and information classes taught across the state in K-12 education.

Link: https://public.education.mn.gov/MDEAnalytics/DataTopic.jsp?TOPICID=84


**American Community Survey**

We accessed the 2019 American Community Survey (ACS) from the tidycensus package in R. The ACS is an annual demographic survey from the U.S. Census Bureau. We used this data set to find by-district information about race, socioeconomic status, living environments, and other relevant variables for our analyses and visualizations. We hypothesized that computer science course availability would be connected to one or more of these variables, due to the trend where more computer science courses are offered in wealthier areas across the country that have more resources. We are using this dataset to further investigate that possibility.


**ACT Data**

We retrieved this data from the Minnesota State Department of Education, which gives us average composite ACT scores for each school district in Minnesota for a range of years. It has been proven that higher ACT scores are connected to wealth, which would lead us to expect that districts with more resources would have higher average ACT scores. As aforementioned, we predict that computer science course availability is greater in areas with more wealth and resources, therefore we thought these scores may be another interesting variable in predicting or visualizing availability of computer science courses by district. In the same way we expect certain demographic information to be correlated with computer science availability, we are hoping to use average ACT score information to present another method of showing the connection.

Link: https://public.education.mn.gov/MDEAnalytics/DataTopic.jsp?TOPICID=87


**Annual Survey of School System Finances**

Another important set of data that we used from the U.S. Census Bureau is the Annual Survey of School System Finances (ASSSF). This data contains information about 2018 financial activity of public elementary and secondary school systems for all states across the country. In particular, we were interested in discovering if funding, revenue, or spending for a district was correlated to availability of computer science courses. Due to the fact that public school funding often comes from income taxes, wealthier areas provide more funding to for their local schools.

Link: https://www.census.gov/programs-surveys/school-finances.html

**Minnesota School Districts**

This data set includes the name of all school districts in Minnesota as well as their district numbers, which was created in 2015. This data came from the Minnesota State Department of Human Services. We joined the district numbers from this data set to the American Community Survey Data, which only had the district names. Accessing district numbers and names was integral in connecting our various datasets, because the common piece of information between our ideas is district information.

Link: https://www.dhs.state.mn.us/main/groups/county_access/documents/pub/dhs16_193591.pdf


# Important Packages and Datasets

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
options(scipen = 999)
```

**Load Packages**

```{r}
library(tidycensus) # for getting the census data
library(tidyverse) # for data cleaning and visualization
library(sf) # for mapping
library(tidymodels)
library(naniar)
library(DALEX)
library(DALEXtra)
library(vip)
library(gridExtra)
library(plotly)
```

**Load Datasets**

```{r}
act <- read_csv("act_data.csv") # ACT
districts <- read_csv("districts.csv") # MN School Districts
course_catalog <- read_csv("course_catalog.csv") # MCCC
ss <- readxl::read_excel("elsec18t.xls") # ASSSF
```

# Data Cleaning

**MCCC Data**

The data from the Department of Education was neatly split into spreadsheets based on course classifications. In order to use this data more easily, we manually combined all the spreadsheets to have one, connected dataset with all the courses offered. We had to manually add in the categories of course classification and category in order to create one dataset. Furthermore, in order to be able to connect to the other datasets, we needed to have the district number as its own variable. In this dataset, the district name and number were combined as one variable, so we used regular expressions to split the name and number. This was fairly simple, due to the consistency in the district variable having the number as the last 7 digits.

```{r}
course_catalog_predictors <-
course_catalog %>%
  mutate(DistNum = as.integer(str_sub(`District`, -7,-4))) %>%
  group_by(DistNum) %>%
  summarise(TotalClasses = sum(n()),
            NumCat = length(unique(Category)))
```



**ACS Data**

The ACS data was already in tidy format, which was convenient for our purposes. We focused on narrowing down variables to the ones that were important to our project and renaming them so that we could easily understand the meaning of the variable. Many of the districts had the word 'Minnesota' in it, so we removed that because all of our data is about Minnesota. Lastly, we altered the dataset to remove any NA values.

```{r}
#census_api_key("key")

mn_2019_census <- get_acs(state = "MN", 
                         geography = "school district (unified)",
                         variables = c("B01003_001",
                                       "B01001_001",
                                       "B01001_004",
                                       "B01001_005",
                                       "B01001_006",
                                       "B02001_001",
                                       "B02001_002",
                                       "B02001_003",
                                       "B02001_005",
                                       "B02001_006",
                                       "B02001_008",
                                       "B19013_001",
                                       "B09010_001",
                                       "B09010_002", 
                                       "B25031_001",
                                       "B25027_001",
                                       "B25027_002",
                                       "B28002_001",
                                       "B28002_002"),
                         geometry = TRUE,
                         year = 2019)

mn_2019_census_nogeom <- mn_2019_census %>%
  select(-`moe`) %>% #get rid of moe variable(not sure what it is)
  spread(variable, estimate) %>% #give each variable a column
  rename(total_pop = B01003_001,
         male_5to9 = B01001_004,
         male_10to14 = B01001_005,
         male_15to17 = B01001_006,
         race_white_only = B02001_002,
         race_Black_only = B02001_003,
         race_Asian_only = B02001_005,
         race_pacific_islander_only = B02001_006,
         at_least_two_races = B02001_008,
         med_household_inc_12mo = B19013_001,
         SSI_pubassist_foodstamps = B09010_002,
         med_gross_rent = B25031_001,
         house_units_w_mortgage = B25027_002,
         internet_subscrip_in_house = B28002_002
         ) %>% #rename variables
  mutate(District = str_extract(`NAME`, "[^,]+")) %>% #delete ", Minnesota" after district name
  select(-NAME) %>% #remove `NAME` column
  mutate(perc_male_5to9 = male_5to9/B01001_001,
         perc_male_10to14 = male_10to14/B01001_001,
         perc_male_15to17 = male_15to17/B01001_001,
         perc_white_only = race_white_only/B02001_001,
         perc_black_only = race_Black_only/B02001_001,
         perc_asian_only = race_Asian_only/B02001_001,
         perc_pacific_islander_only = race_pacific_islander_only/B02001_001,
         perc_at_least_two_races = at_least_two_races/B02001_001,
         perc_SSI_pubassist_foodstamps = SSI_pubassist_foodstamps/B09010_001,
         perc_house_units_w_mortgage = house_units_w_mortgage/B25027_001,
         perc_internet_subscription = internet_subscrip_in_house/B28002_001
         ) %>% #create percentage variables
  select(-B01001_001,-B02001_001, -B09010_001, -B25027_001, -B28002_001 ) #deselect variables used to calculate % but aren't important for our visualizations
```


**ACT Data**

To begin with, this dataset included a range of years, so we filtered the graduation year to our year of interest, which was 2018. This dataset also originally had extra variables that were irrelevant to our interests, so our cleaning process involved removing predictors so that we were only left with district name and average composite ACT score. The district name included the district number, so to create two separate values we used regular expressions. The district number was consistently located at the same spot in the name.

```{r}
act_clean <-
act %>%
  filter(`Grad Year` == 2018,
         `Analysis Level` == "District",
         `District Name` != "MINNESOTA DEPT OF EDUCATION",
         `Avg Comp` != ".") %>%
  select(`District Name`, `Avg Comp`) %>%
  mutate(`Dist Num` = as.integer(str_extract_all(`District Name`, "[:digit:]+"))) %>%
  mutate(`Dist Num`= replace(`Dist Num`, `District Name`=="GRANADA-HUNTLEY-EAST CHAIN SD", 2536),
         `Dist Num`= replace(`Dist Num`, `District Name`=="ALBERT LEA AREA SCHOOLS", 241),
         `Dist Num`= replace(`Dist Num`, `District Name`=="ATWATER-COSMOS-GROVE CITY SD", 2396),
         `Dist Num`= replace(`Dist Num`, `District Name`=="BOLD SCHOOL DISTRICT", 2534),
         `Dist Num`= replace(`Dist Num`, `District Name`=="BUFFALO HANOVER MONTROSE SD", 0877),
         `Dist Num`= replace(`Dist Num`, `District Name`=="BUFFALO LAKE-HECTOR-STEWART SD", 2159),
         `Dist Num`= replace(`Dist Num`, `District Name`=="EASTERN CARVER CO SCHOOLS", 112),
         `Dist Num`= replace(`Dist Num`, `District Name`=="MANKATO AREA PUBLIC SCHS", 77),
         `Dist Num`= replace(`Dist Num`, `District Name`=="MINNEAPOLIS PUBLIC SCH DIST", 9991),
         `Dist Num`= replace(`Dist Num`, `District Name`=="MINNEOTA PUBLIC SCHOOLS", 414),
         `Dist Num`= replace(`Dist Num`, `District Name`=="PLAINVIEW ELGIN MILLVILLE SD", 2899),
         `Dist Num`= replace(`Dist Num`, `District Name`=="ROYALTON PUBLIC SCHOOLS", 485),
         `Dist Num`= replace(`Dist Num`, `District Name`=="WASECA PUBLIC SCHOOL DIST", 829),
         `Dist Num`= replace(`Dist Num`, `District Name`=="WAUBUN-OGEMA-WHITE EARTH PSD", 435),
         `Dist Num`= replace(`Dist Num`, `District Name`=="WINONA AREA PUBLIC SCHOOL DIST", 861))
```


**Annual Survey of School System Finances**

Originally, this dataset includes information about all the states. Part of cleaning was filtering for just the state of Minnesota, since that is our target region.

CHECK WITH THY

```{r}
ss %>%
  # filter for MN
  filter(str_detect(NCESID, "^27")) %>%
  arrange(NCESID) %>%
  head(10) %>%
  select(NCESID)
```


**Minnesota School Districts**

This dataset originally had four categories, district name, district number, start date, and end date. To prepare this dataset we removed the start and end date variables. In order to join it with the ACS dataset, we had to compare the district names in both to ensure that they aligned. Only 16 district names didn't match, which was a small number, so we manually changed the district names to match the names in the ACS dataset.


# Data Joining

Part of the work we did after finding, importing, and cleaning our various datasets was joining them together. An important piece for this was ensuring the district names and numbers aligned, and most of that was completed in the previous step. Here is our code for joining the datasets:

```{r}
# read in datasets
## annual survey of school system finances (2018, national)
ss <- readxl::read_excel("elsec18t.xls")

## selected variables from the ACS (2019, MN)
mn_acs <- st_read("mn_2019_census/mn_2019_census.shp", 
                  geometry_column = "geometry", 
                  fid_column_name = "geometry")
mn_acs <- as.data.frame(mn_acs)
# rename the columns
names(mn_acs) <- c("GEOID", "male_5to9", "male_10to14", "male_15to17",
                   "total_pop", "race_white_only", "race_Black_only", "race_Asian_only",
                   "race_pacific_islander_only", "at_least_two_races", "SSI_pubassist_foodstamps",
                   "med_household_inc_12mo", "house_units_w_mortgage", "med_gross_rent",
                   "internet_subscrip_in_house", "District", "perc_male_5to9", "perc_male_10to14",
                   "perc_male_15to17", "perc_white_only", "perc_black_only", "perc_asian_only",
                   "perc_pacific_islander_only", "perc_at_least_two_races", "perc_SSI_pubassist_foodstamps",
                   "perc_house_units_w_mortgage", "perc_internet_subscription",
                   "District.Nbr", "geometry")
# move the district name and number cols to the front
mn_acs <- mn_acs %>%
  relocate(District, District.Nbr, .after = GEOID)

## ACT scores (MN)
mn_act <- read.csv("act_clean.csv")[-1] # remove the redundant index column created when reading by csv
```

Join by NCES ID for school districts

```{r}
# look at this column from the school survey
ss %>%
  # filter for MN 
  filter(str_detect(NCESID, "^27")) %>%
  arrange(NCESID) %>%
  head(10) %>%
  select(NCESID)

# and from the ACS
mn_acs %>%
  arrange(GEOID) %>%
  head(10) %>%
  select(GEOID)
```

```{r}
# left join on ACS data
mn_acs_ss <- mn_acs %>%
  left_join(ss, by = c("GEOID" = "NCESID"))
```

Let's look at the name columns from the two datasets to verify that they point to the same district

```{r}
mn_acs_ss %>%
  select(District, NAME)
```

Now let's see which rows from the ACS and school survey didn't get a match

```{r}
# ACS
mn_acs_ss %>%
  filter(is.na(NAME))

# school survey
ss %>%
  filter(str_detect(NCESID, "^27")) %>% # filter for MN
  anti_join(mn_acs, by = c("NCESID" = "GEOID"))
```

So only the row "Remainder of Minnesota" from the ACS didn't have a match, which makes perfect sense. There are 67 school districts from the school survey that did not join to the ACS, but perhaps they fall into the "Remainder of Minnesota" category. A lot of these districts also had 0 for enrollment.

To join this dataset with the ACT data, we'll have to use the school district numbers as the ACT data does not contain the NCES ID.

```{r}
# look at this variable from each dataset to see their format
mn_acs_ss %>%
  arrange(District.Nbr) %>%
  select(District.Nbr)

mn_act %>%
  arrange(Dist.Num) %>%
  select(Dist.Num)
```

Seems like we're missing ACT scores for 11 school districts. Nevertheless the formats do match so let's join the datasets

```{r}
mn_acs_ss_act <- mn_acs_ss %>%
  mutate(District.Nbr = as.integer(District.Nbr)) %>%
  left_join(mn_act, by = c("District.Nbr" = "Dist.Num"))
```

Finally before we export the data, we can remove the redundant names and ID columns from the school survey and ACT datasets as we'll stick to their format from the ACS

```{r}
mn_acs_ss_act <- mn_acs_ss_act %>%
  select(-c("IDCENSUS", "NAME", "District.Name"))
```

Export the data:

```{r}
#write.csv(mn_acs_ss_act, "mn_acs_ss_act.csv")
```

```{r}
st_write(mn_acs_ss_act, "mn_acs_ss_act/mn_acs_ss_act.shp", 
         append=FALSE, 
         fid_column_name = "geometry")
```

Join with predictor data, replace all missing data from the course catalog with 0, and remove the redundant variable. This is the final step!
```{r}
mn_acs_ss_act_pred <- mn_acs_ss_act %>%
  # mutate(District.Nbr = as.integer(District.Nbr)) %>% ## no longer necessary since they're already ints
  ## changed to left join to ignore rows without matches from the course catalog
  left_join(course_catalog_predictors, by = c("District.Nbr" = "DistNum")) %>% 
  mutate(TotalClasses = ifelse(is.na(TotalClasses), 0, TotalClasses),
         NumCat = ifelse(is.na(NumCat), 0, NumCat)) 
```

# Mapping and Shape Files

While we had all the data necessary to create useful graphs and tables, one of our goals was to create visualizations and maps. Our data is focused on the state of Minnesota and all of it segmented by school districts, therefore we believed an interesting and informative way to present this data is with maps. The piece that was missing for us to accomplish this were the shape files for the districts. Shape files are notoriously tricky, and implementing this was a challenge.

- why a challenge?

```{r}
##### RUN THIS CHUNK WHENEVER YOU READ IN THE DATA
mn_acs_ss_act_pred <- st_read("mn_acs_ss_act_pred/mn_acs_ss_act_pred.shp",
                geometry_column = "geometry",
                fid_column_name = "geometry",
                quiet = TRUE)
# rename the census variables because their names were reformatted...
names(mn_acs_ss_act_pred)[1:28] <- 
  c("GEOID", "District", "District.Nbr", "male_5to9", "male_10to14", "male_15to17",
    "total_pop", "race_white_only", "race_Black_only", "race_Asian_only",
    "race_pacific_islander_only", "at_least_two_races", "SSI_pubassist_foodstamps",
    "med_household_inc_12mo", "house_units_w_mortgage", "med_gross_rent",
    "internet_subscrip_in_house", "perc_male_5to9", "perc_male_10to14",
    "perc_male_15to17", "perc_white_only", "perc_black_only", "perc_asian_only",
    "perc_pacific_islander_only", "perc_at_least_two_races", "perc_SSI_pubassist_foodstamps",
    "perc_house_units_w_mortgage", "perc_internet_subscription")
names(mn_acs_ss_act_pred)[93] <- c("TotalClasses")
```


# Visualizations and Plotly

Although we knew our maps were sufficient, we wanted to add another element to our project. We decided to make the maps interactive and chose to use plotly to achieve this. The plotly functions had an issue with our final dataset and would not map unless we removed all rows with NA; converting the NA values to 0 did not fix the issue, and neither did converting the ‘N’ values we found in two variable columns to 0. In total, dropping the NA rows removed 11 districts. With more time this issue should be explored further. One thing that is important to note is that the districts that were not mapped still show interactivity based on whichever district is closest to the pointer.

Static Graph
```{r}
#population
ggplot(mn_acs_ss_act_pred, aes(fill = total_pop)) + 
  geom_sf(color = NA) + 
  scale_fill_viridis_c(option = "magma", labels = scales::comma) +
  labs(title = "Total Population", fill = "") +
  theme(axis.ticks = element_blank(),
        axis.text = element_blank(),
        panel.background = element_rect(fill = "white"),
        plot.title = element_text(hjust = 0.5))
```

Plotly
```{r}
mn_acs_ss_act_pred_viz <- 
  mn_acs_ss_act_pred %>%  drop_na()

ggplotly(
  ggplot(mn_acs_ss_act_pred_viz) +
  geom_sf(aes(fill = TotalClasses, text = paste(District)),  size = .1, color = "white") +
  scale_fill_viridis_c(option = "B") +
  labs(title = "Total CS Courses Offered", fill = "") +
  theme(axis.ticks = element_blank(),
        axis.text = element_blank(),
        panel.background = element_rect(fill = "white"),
        plot.title = element_text(hjust = 0.5),
        title =element_text(size=12, face='bold'))
) 
```


# Modeling

The goal of our model was to predict the number of computer science courses offered per district based on the variables from our ACS, ASSSF, and ACT data.

```{r paged-table, echo=FALSE}
library(knitr)
knit_print.data.frame <- function(x, ...) {
  asis_output(
    rmarkdown:::paged_table_html(x, options = attr(x, "options")),
    meta = list(dependencies = rmarkdown:::html_dependency_pagedtable())
  )
}
registerS3method("knit_print", "data.frame", knit_print.data.frame)
```

We first read in the data, cleaning up variable names as they were abbreviated once exported to shapefile format.

```{r}
# read in the data
mn <- st_read("mn_acs_ss_act_pred/mn_acs_ss_act_pred.shp",
              geometry_column = "geometry",
              fid_column_name = "geometry")
# rename the census variables because their names were reformatted...
names(mn)[1:28] <- 
  c("GEOID", "District", "District.Nbr", "male_5to9", "male_10to14", "male_15to17",
    "total_pop", "race_white_only", "race_Black_only", "race_Asian_only",
    "race_pacific_islander_only", "at_least_two_races", "SSI_pubassist_foodstamps",
    "med_household_inc_12mo", "house_units_w_mortgage", "med_gross_rent",
    "internet_subscrip_in_house", "perc_male_5to9", "perc_male_10to14",
    "perc_male_15to17", "perc_white_only", "perc_black_only", "perc_asian_only",
    "perc_pacific_islander_only", "perc_at_least_two_races", "perc_SSI_pubassist_foodstamps",
    "perc_house_units_w_mortgage", "perc_internet_subscription")
names(mn)[93] <- c("TotalClasses")
```

### Data Exploration

```{r}
# look at the variables and their types
str(mn)
```

Looking at the variables and their types, we were dealing with predominantly numerical variables. `GEOID`, `District`, `District.Nbr`, `CONUM`, `CSA`, and `CBSA` are ID variables so in the recipe we'll have to specify that so they wouldn't be included in the model.

We decided to drop the geometry column and convert the data to a regular data frame format for this part because it was not necessary for the models and including it led to issues running some functions.

```{r}
mn <- mn %>%
  st_drop_geometry()
```

We plotted the distributions of our variables to see what kind of data transformation we would need to perform.

Below are sets of our predictors, first from the Annual Survey of School System Finances:

```{r expl-cont-ss, fig.width=12, fig.height=8}
mn %>%
  select(-GEOID) %>%
  # variables from the Annual Survey of School System Finances are in all caps
  select(matches("^[A-Z]{4,}", ignore.case = FALSE)) %>%
  select(where(is.numeric)) %>%
  pivot_longer(cols = everything(),
               names_to = "variable",
               values_to = "values") %>%
  ggplot(aes(x = values)) +
  geom_histogram() +
  facet_wrap(vars(variable),
             scales = "free")
```

and now from the ACS and ACT data:

```{r expl-cont-acs-act, fig.width=12, fig.height=8}
mn %>%
  # remove the ID variables
  select(-c("GEOID", "District", "District.Nbr")) %>%
  # these variables have at least one lower case character
  select(matches("[a-z]", ignore.case = FALSE)) %>%
  # filter for numerical variables
  select(where(is.numeric)) %>%
  pivot_longer(cols = everything(),
               names_to = "variable",
               values_to = "values") %>%
  ggplot(aes(x = values)) +
  geom_histogram() +
  facet_wrap(vars(variable),
             scales = "free")
```

A lot of the data are right-skewed, which makes sense as many variables are raw counts.

We also checked for missing data:

```{r missing-data}
mn %>% 
  add_n_miss() %>% 
  count(n_miss_all)
```

We had one observation missing 65 variables, and this was the "Remainder of Minnesota" observation from the ACS. The NAs caused issues when we tried to fit a LASSO and random forest model so we removed them beforehand.

```{r}
mn <- mn %>%
  drop_na()
```

### Model Recipe

```{r include=FALSE}
# set the seed
set.seed(21)
```

From our data exploration, these were the things we had to do in the recipe:

* Log-transform most of the variables (except for the ones that start with PCT from the School Survey as they're percentages and PP as they're spending per pupil and not right-skewed)
* Use percentages for ACS variables
* Normalize all numerical variables
* Ignore `PCTTOTA` and `LOCRPAR` as all variables have the same value
* Make the ID variables evaluative (i.e. not included in modeling)

**Split data**

We split the data into a training and testing set and also created cross-validation folds for evaluation.

```{r split}
# split the data into a training and test set
mn_split <- initial_split(mn, prop = .75)
mn_training <- training(mn_split)
mn_testing <- testing(mn_split)
```

```{r cv}
# create cross-validation folds
mn_cv <- vfold_cv(mn_training, v = 5)
```

**Preprocess data**

Here is the recipe for our models:

```{r recipe}
mn_recipe <- 
  recipe(TotalClasses ~ ., data = mn_training) %>%
  # ignore observations with missing data (necessary for LASSO mod)
  step_naomit(everything(), skip = TRUE) %>%
  # remove variables
  step_rm(
    # this one can be considered as response variable itself
    NumCat, 
    # all variables have the same value for these two
    PCTTOTA, LOCRPAR, 
    # raw counts from ACS
    matches("[a-z]", ignore.case = FALSE),
    -starts_with("perc"),
    -total_pop,
    -Avg_Cmp,
    -TotalClasses
    ) %>%
  # log-transform 
  step_log(
    # total population
    total_pop,
    # spending / revenue variables from the school survey
    ## ignore those that start with P since they're percentages / spending per student
    matches("^[A-OQ-Z]{4,}", ignore.case = FALSE), 
    ## ignore ID variables as well
    -GEOID, -CONUM, -CBSA,
    # some variables have 0s which will produce NaNs when log-transformed
    offset = 1) %>% 
  # make ID variables evaluative (not included in modeling)
  update_role(
    all_of(c("GEOID",
             "District",
             "District.Nbr",
             "CONUM",
             "CSA",
             "CBSA")),
    new_role = "evaluative") %>%
  # make integers numeric
  step_mutate_at(is.integer, fn = as.numeric) %>%
  # normalize numerical variables
  step_normalize(all_predictors())
```

And here is what the data looked like post-transformation:

```{r}
mn_recipe %>%
  prep(mn_training) %>%
  juice()
```

### Model Fitting

**Regular linear regression**

We first tested a regular linear regression model and looked at the table of coefficients. Since we had an overwhelming number of predictors, we assumed beforehand that this model would not perform well due to overfitting.

```{r lm}
# define the model type
mn_linear_mod <-
  linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")

# set up the workflow
mn_lm_wf <-
  workflow() %>%
  add_recipe(mn_recipe) %>%
  add_model(mn_linear_mod)

# fit the model
mn_lm_fit <-
  mn_lm_wf %>%
  fit(mn_training)

# display the results
mn_lm_fit %>% 
  pull_workflow_fit() %>% 
  tidy() %>% 
  mutate(across(where(is.numeric), ~round(.x,3))) 
```

**LASSO**

To deal with this, we switched to LASSO to shrink coefficients to zero and thereby eliminate insignificant variables from the model.

```{r lasso}
# define the model type
mn_lasso_mod <-
  linear_reg(mixture = 1) %>%
  set_engine("glmnet") %>%
  set_args(penalty = tune()) %>%
  set_mode("regression")

# set up the workflow
mn_lasso_wf <-
  workflow() %>%
  add_recipe(mn_recipe) %>%
  add_model(mn_lasso_mod)

# set up penalty grid for tuning
penalty_grid <- grid_regular(penalty(),
                             levels = 20)

# tune the parameter
mn_lasso_tune <-
  mn_lasso_wf %>%
  tune_grid(
    resamples = mn_cv,
    grid = penalty_grid
  )
```

We chose the best parameter based on RMSE and finalized the workflow / model. Then we looked at the variables that were retained by LASSO.

```{r}
# show the best penalty parameter
mn_lasso_tune %>% 
  show_best(metric = "rmse")

# select best parameter by smallest rmse
(best_param <- mn_lasso_tune %>% 
    select_best(metric = "rmse"))
```

```{r}
# finalize workflow
mn_lasso_final_wf <- mn_lasso_wf %>% 
  finalize_workflow(best_param)

# fit final model
mn_lasso_final_mod <-
  mn_lasso_final_wf %>%
  fit(data = mn_training)

# look at the table of coefficients
mn_lasso_final_mod %>% 
  pull_workflow_fit() %>% 
  tidy()  %>%
  # filter for predictors with non-zero coefficients 
  filter(estimate != 0)
```

**Random forest**

Our last candidate was a random forest model.

```{r}
# define the model type
mn_rf_mod <- 
  rand_forest(mtry = 23, # ~1/3 of predictors 
              min_n = 5, 
              trees = 200) %>% 
  set_mode("regression") %>% 
  set_engine("ranger")

# set up the workflow
mn_rf_wf <-
  workflow() %>%
  add_recipe(mn_recipe) %>%
  add_model(mn_rf_mod)

# fit the model
mn_rf_fit <- 
  mn_rf_wf %>% 
  fit(mn_training)
```


### Model evaluation and comparison
 
```{r}
# fit model with best tuning parameter(s) to training data and apply to test data
mn_lm_test <- 
  mn_lm_wf %>% 
  last_fit(mn_split)
mn_lasso_test <- 
  mn_lasso_final_wf %>% 
  last_fit(mn_split)
mn_rf_test <-
  mn_rf_wf %>%
  last_fit(mn_split)
```

Of our three models, the random forest performed best, followed by LASSO and then regular linear regression.

```{r}
# collect metrics for model applied to test data
mn_lm_test %>%
  collect_metrics
mn_lasso_test %>% 
  collect_metrics()
mn_rf_test %>% 
  collect_metrics()
```

**Residuals**

Since the RMSE from the regular lm model was rather high, we decided to only compare the LASSO and random forest moving forward. We computed their overall performance metrics and looked at the residuals.

```{r}
lasso_explain <- 
  explain_tidymodels(
    model = mn_lasso_final_mod,
    data = mn_training %>% select(-TotalClasses), 
    y = mn_training %>%  pull(TotalClasses),
    label = "lasso"
  )
```

```{r}
rf_explain <- 
  explain_tidymodels(
    model = mn_rf_fit,
    data = mn_training %>% select(-TotalClasses), 
    y = mn_training %>%  pull(TotalClasses),
    label = "rf"
  )
```

```{r}
# get overall performance metrics
lasso_mod_perf <- model_performance(lasso_explain)
rf_mod_perf <-  model_performance(rf_explain)
```

Here are tables of their performance metrics:

```{r}
# LASSO
data.frame(lasso_mod_perf$measures)
# random forest
data.frame(rf_mod_perf$measures)
```

and the distribution of the residuals:

```{r}
plot(lasso_mod_perf,
     rf_mod_perf, 
     geom = "boxplot")
```

**Variable importance**

In the end the random forest showed to greatly outperform the LASSO. Our final step was to look at the variable importance plot from this final model.

```{r}
set.seed(1) 
# create explainer
rf_explain <- 
  explain_tidymodels(
    model = mn_rf_fit,
    data = mn_training %>% select(-TotalClasses), 
    y = mn_training %>%  pull(TotalClasses),
    label = "rf"
  )
# compute variable importance
rf_var_imp <- model_parts(rf_explain)
# plot
plot(rf_var_imp, show_boxplots = TRUE)
```


# Implications

The first thing that is critical to highlight is that correlation does not imply causation. Although this project looks at connections between various datasets and different variables, we are not suggesting that any of our predictors directly alter computer science course availability. It is possible, given the work that we have done, but without an experiment we cannot be certain about causation.

Previous work does exist about how disparities in education are related to many of the variables we displayed in our project, such as household income and race. It is an intersectional issue that combines many layers of societal disparities. For instance, it has been proven that ACT and standardized test scores show more about family wealth and privilege than actual intelligence or likelihood for success. Therefore, it is logical that the same districts that have high average ACT scores will have high median household incomes.

That being said, there are many nuances to this issue that we could not address within the scope of our project. One variable we looked at was race, and while the connection between race and educational disparities has been studied, that can be difficult to see in some of our work. We hypothesize that there are a few reasons for this. First, Minnesota in general is largely populated by White people. Furthermore, the place where there is the most diversity, near the Twin Cities, is also a place with considerable inequality. Without this information, it may seem as though there is correlation between greater diversity, higher median household income, and computer science course availability. However, we cannot make this claim without further investigating how the inequalities in each district play a role.

Along with that, the population size of the districts could affect the outcomes. Districts can encompass many schools, and it is possible that within a district there is variation in the data. Future work might include investigating a smaller region to explore some of these nuances in order to better understand the connection between computer science course availability in K-12 education and our other variables.

---
title: "Final Project - Behind The Scenes"
author: "Anael Kuperwajs Cohen, Colleen Minnihan, Hayley Hadges, Thy Nguyen"
date: "5/05/2021"
output: html_document
---

# Introduction

Computer Science is a field that is growing rapidly in the United States and around the world today. Advancements in computer science from industry are constantly being released and technology is becoming more ingrained into our daily lives. The increasing demand of computer scientists had caused the occupation to grow in popularity. To meet the demand, educational institutions and systems are increasing the amount of courses offered in order to train more future computer scientists. This development started at the college level, where majoring in computer science is becoming a widely available option. At Macalester College, it is one of the largest departments for both students and faculty. While the availability of courses at the college level is an amazing start, there is a big push to have computer sciences courses offered in K-12 education. Offering computer science courses in elementary and secondary schools provides an opportunity for kids to expose themselves to coding. This could lead younger students to discover new interests and get engaged with computer science earlier. Often, being exposed to computer science at a younger age can make students more comfortable with the material and the field later on. This can lead to a more empowered and diverse set of students entering the workforce or higher education. Given the importance of having computer science courses available in K-12 education, we decided to explore the availability of computer science courses in K-12 school districts in Minnesota. For our Advanced Data Science final project, we will explore the connection between a variety of datasets related to this topic, including K-12 computer science course availability in Minnesota, demographic information from the U.S. census, ACT scores, and funding.


# About the Datasets

**Minnesota Common Course Catalogue**

This data set is provided by the Minnesota State Department of Education and is available to the public. The Minnesota Common Course Catalogue (MCCC) presents information about all the courses that are offered across school districts in Minnesota. For our purposes, we are looking at subject area 10, computer and information sciences (K-12), and all categories and classifications. For each course classification, there is information about the district where it is taught, the region of the state, the local course title, whether it is an AP or IB course, if it fulfills a graduation requirement, among others. We joined this dataset with information about what category the course classification falls into, such as A - computer literacy, B - management information systems, or C - network systems. The final dataset has 1,602 computer and information classes taught across the state in K-12 education.

Link: https://public.education.mn.gov/MDEAnalytics/DataTopic.jsp?TOPICID=84


**American Community Survey**

We accessed the 2019 American Community Survey (ACS) from the tidycensus package in R. The ACS is an annual demographic survey from the U.S. Census Bureau. We used this data set to find by-district information about race, socioeconomic status, living environments, and other relevant variables for our analyses and visualizations. We hypothesized that computer science course availability would be connected to one or more of these variables, due to the trend where more computer science courses are offered in wealthier areas across the country that have more resources. We are using this dataset to further investigate that possibility.


**ACT Data**

We retrieved this data from the Minnesota State Department of Education, which gives us average composite ACT scores for each school district in Minnesota for a range of years. It has been proven that higher ACT scores are connected to wealth, which would lead us to expect that districts with more resources would have higher average ACT scores. As aforementioned, we predict that computer science course availability is greater in areas with more wealth and resources, therefore we thought these scores may be another interesting variable in predicting or visualizing availability of computer science courses by district. In the same way we expect certain demographic information to be correlated with computer science availability, we are hoping to use average ACT score information to present another method of showing the connection.

Link: https://public.education.mn.gov/MDEAnalytics/DataTopic.jsp?TOPICID=87


**Annual Survey of School System Finances**

Another important set of data that we used from the U.S. Census Bureau is the Annual Survey of School System Finances (ASSSF). This data contains information about 2018 financial activity of public elementary and secondary school systems for all states across the country. In particular, we were interested in discovering if funding, revenue, or spending for a district was correlated to availability of computer science courses. Due to the fact that public school funding often comes from income taxes, wealthier areas provide more funding to for their local schools.

Link: https://www.census.gov/programs-surveys/school-finances.html

**Minnesota School Districts**

This data set includes the name of all school districts in Minnesota as well as their district numbers, which was created in 2015. This data came from the Minnesota State Department of Human Services. We joined the district numbers from this data set to the American Community Survey Data, which only had the district names. Accessing district numbers and names was integral in connecting our various datasets, because the common piece of information between our ideas is district information.

Link: https://www.dhs.state.mn.us/main/groups/county_access/documents/pub/dhs16_193591.pdf


#Important Packages and Datasets

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
options(scipen = 999)
```

## Load Packages

```{r}
library(tidycensus) # for getting the census data
library(tidyverse) # for data cleaning and visualization
library(sf) # for mapping
library(tidymodels)
library(naniar)
library(DALEX)
library(DALEXtra)
library(vip)
```

## Load Datasets

```{r}
act <- read_csv("act_data.csv") # ACT
districts <- read_csv("districts.csv") # MN School Districts
course_catalog <- read_csv("course_catalog.csv") # MCCC
ss <- readxl::read_excel("elsec18t.xls") # ASSSF
```


# Data Cleaning

**MCCC Data**

The data from the Department of Education was neatly split into spreadsheets based on course classifications. In order to use this data more easily, we manually combined all the spreadsheets to have one, connected dataset with all the courses offered. We had to manually add in the categories of course classification and category in order to create one dataset. Furthermore, in order to be able to connect to the other datasets, we needed to have the district number as its own variable. In this dataset, the district name and number were combined as one variable, so we used regular expressions to split the name and number. This was fairly simple, due to the consistency in the district variable having the number as the last 7 digits.

```{r}
# course_catalog_predictors <-
# course_catalog %>% 
#   mutate(DistNum = as.integer(str_sub(`District`, -7,-4))) %>% 
#   group_by(DistNum) %>% 
#   summarise(TotalClasses = sum(n()),
#             NumCat = length(unique(Category)))
```



**ACS Data**

The ACS data was already in tidy format, which was convenient for our purposes. We focused on narrowing down variables to the ones that were important to our project and renaming them so that we could easily understand the meaning of the variable. Many of the districts had the word 'Minnesota' in it, so we removed that because all of our data is about Minnesota. Lastly, we altered the dataset to remove any NA values.

```{r}
# mn_2019_census <- get_acs(state = "MN", 
#                          geography = "school district (unified)",
#                          variables = c("B01003_001",
#                                        "B01001_004",
#                                        "B01001_005",
#                                        "B01001_006",
#                                        "B02001_002",
#                                        "B02001_003",
#                                        "B02001_005",
#                                        "B02001_006",
#                                        "B02001_008",
#                                        "B19013_001",
#                                        "B09010_001", 
#                                        "B25031_001",
#                                        "B25027_001",
#                                        "B28003_001"),
#                          geometry = TRUE,
#                          year = 2019)
# 
# mn_2019_census_nogeom <- mn_2019_census %>%
#   st_drop_geometry() %>% #get rid of the geometry so we can spread()
#   select(-`moe`) %>% #get rid of moe variable(not sure what it is)
#   spread(variable, estimate) %>% #give each variable a column
#   rename(total_pop = B01003_001,
#          male_5to9 = B01001_004,
#          male_10to14 = B01001_005,
#          male_15to17 = B01001_006,
#          race_white_only = B02001_002,
#          race_Black_only = B02001_003,
#          race_Asian_only = B02001_005,
#          race_pacific_islander_only = B02001_006,
#          at_least_two_races = B02001_008,
#          med_household_inc_12mo = B19013_001,
#          SSI_pubassist_foodstamps = B09010_001,
#          med_gross_rent = B25031_001,
#          house_units_w_mortgage = B25027_001,
#          comp_internet_subscrip_in_house = B28003_001
#          ) %>% #rename variables
#   mutate(District = str_extract(`NAME`, "[^,]+")) %>% #delete ", Minnesota" after district name
#   select(-NAME)#remove `NAME` column
# 
# #adding a column with district numbers
# districts <- districts %>%
#   mutate(District = `District Name`) %>%
#   select(-`X3`, - `District Name`)
# 
# mn_2019_census_nogeom <- mn_2019_census_nogeom %>%
#   left_join(districts)
```


**ACT Data**

To begin with, this dataset included a range of years, so we filtered the graduation year to our year of interest, which was 2018. This dataset also originally had extra variables that were irrelevant to our interests, so our cleaning process involved removing predictors so that we were only left with district name and average composite ACT score. The district name included the district number, so to create two separate values we used regular expressions. The district number was consistently located at the same spot in the name.

```{r}
# act_clean <-
# act %>%
#   filter(`Grad Year` == 2018,
#          `Analysis Level` == "District",
#          `District Name` != "MINNESOTA DEPT OF EDUCATION",
#          `Avg Comp` != ".") %>%
#   select(`District Name`, `Avg Comp`) %>%
#   mutate(`Dist Num` = as.integer(str_extract_all(`District Name`, "[:digit:]+"))) %>%
#   mutate(`Dist Num`= replace(`Dist Num`, `District Name`=="GRANADA-HUNTLEY-EAST CHAIN SD", 2536),
#          `Dist Num`= replace(`Dist Num`, `District Name`=="ALBERT LEA AREA SCHOOLS", 241),
#          `Dist Num`= replace(`Dist Num`, `District Name`=="ATWATER-COSMOS-GROVE CITY SD", 2396),
#          `Dist Num`= replace(`Dist Num`, `District Name`=="BOLD SCHOOL DISTRICT", 2534),
#          `Dist Num`= replace(`Dist Num`, `District Name`=="BUFFALO HANOVER MONTROSE SD", 0877),
#          `Dist Num`= replace(`Dist Num`, `District Name`=="BUFFALO LAKE-HECTOR-STEWART SD", 2159),
#          `Dist Num`= replace(`Dist Num`, `District Name`=="EASTERN CARVER CO SCHOOLS", 112),
#          `Dist Num`= replace(`Dist Num`, `District Name`=="MANKATO AREA PUBLIC SCHS", 77),
#          `Dist Num`= replace(`Dist Num`, `District Name`=="MINNEAPOLIS PUBLIC SCH DIST", 9991),
#          `Dist Num`= replace(`Dist Num`, `District Name`=="MINNEOTA PUBLIC SCHOOLS", 414),
#          `Dist Num`= replace(`Dist Num`, `District Name`=="PLAINVIEW ELGIN MILLVILLE SD", 2899),
#          `Dist Num`= replace(`Dist Num`, `District Name`=="ROYALTON PUBLIC SCHOOLS", 485),
#          `Dist Num`= replace(`Dist Num`, `District Name`=="WASECA PUBLIC SCHOOL DIST", 829),
#          `Dist Num`= replace(`Dist Num`, `District Name`=="WAUBUN-OGEMA-WHITE EARTH PSD", 435),
#          `Dist Num`= replace(`Dist Num`, `District Name`=="WINONA AREA PUBLIC SCHOOL DIST", 861))
```


**Annual Survey of School System Finances**

Originally, this dataset includes information about all the states. Part of cleaning was filtering for just the state of Minnesota, since that is our target region.

```{r}
# ss %>%
#   # filter for MN 
#   filter(str_detect(NCESID, "^27")) %>%
#   arrange(NCESID) %>%
#   head(10) %>%
#   select(NCESID)
```


**Minnesota School Districts**

This dataset originally had four categories, district name, district number, start date, and end date. To prepare this dataset we removed the start and end date variables. In order to join it with the ACS dataset, we had to compare the district names in both to ensure that they aligned. Only 16 district names didn't match, which was a small number, so we manually changed the district names to match the names in the ACS dataset.


# Data Joining

Part of the work we did after finding, importing, and cleaning our various datasets was joining them together. An important piece for this was ensuring the district names and numbers aligned, and most of that was completed in the previous step. Here is our code for joining the datasets:

```{r}

```


# Mapping and Shape Files

While we had all the data necessary to create useful graphs and tables, one of our goals was to create visualizations and maps. Our data is focused on the state of Minnesota and all of it segmented by school districts, therefore we believed an interesting and informative way to present this data is with maps. The piece that was missing for us to accomplish this were the shape files for the districts. Shape files are notoriously tricky, and implementing this was a challenge.

- why a challenge?

# Visualizations and Plotly

Although we knew our maps were sufficient, we wanted to add another element to our project. We decided to make the maps interactive and chose to use plotly to achieve this.


# Modeling

The goal of our model was to predict the number of computer science courses offered per district based on the variables from our ACS, ASSSF, and ACT data.

```{r paged-table, echo=FALSE}
library(knitr)
knit_print.data.frame <- function(x, ...) {
  asis_output(
    rmarkdown:::paged_table_html(x, options = attr(x, "options")),
    meta = list(dependencies = rmarkdown:::html_dependency_pagedtable())
  )
}
registerS3method("knit_print", "data.frame", knit_print.data.frame)
```

We first read in the data, cleaning up variable names as they were abbreviated once exported to shapefile format.

```{r}
# read in the data
mn <- st_read("mn_acs_ss_act_pred/mn_acs_ss_act_pred.shp",
              geometry_column = "geometry",
              fid_column_name = "geometry")
# rename the census variables because their names were reformatted...
names(mn)[1:28] <- 
  c("GEOID", "District", "District.Nbr", "male_5to9", "male_10to14", "male_15to17",
    "total_pop", "race_white_only", "race_Black_only", "race_Asian_only",
    "race_pacific_islander_only", "at_least_two_races", "SSI_pubassist_foodstamps",
    "med_household_inc_12mo", "house_units_w_mortgage", "med_gross_rent",
    "internet_subscrip_in_house", "perc_male_5to9", "perc_male_10to14",
    "perc_male_15to17", "perc_white_only", "perc_black_only", "perc_asian_only",
    "perc_pacific_islander_only", "perc_at_least_two_races", "perc_SSI_pubassist_foodstamps",
    "perc_house_units_w_mortgage", "perc_internet_subscription")
names(mn)[93] <- c("TotalClasses")
```

## Data Exploration

```{r}
# look at the variables and their types
str(mn)
```

Looking at the variables and their types, we were dealing with predominantly numerical variables. `GEOID`, `District`, `District.Nbr`, `CONUM`, `CSA`, and `CBSA` are ID variables so in the recipe we'll have to specify that so they wouldn't be included in the model.

We decided to drop the geometry column and convert the data to a regular data frame format for this part because it was not necessary for the models and including it led to issues running some functions.

```{r}
mn <- mn %>%
  st_drop_geometry()
```

We plotted the distributions of our variables to see what kind of data transformation we would need to perform.

Below are sets of our predictors, first from the Annual Survey of School System Finances:

```{r expl-cont-ss, fig.width=12, fig.height=8}
mn %>%
  select(-GEOID) %>%
  # variables from the Annual Survey of School System Finances are in all caps
  select(matches("^[A-Z]{4,}", ignore.case = FALSE)) %>%
  select(where(is.numeric)) %>%
  pivot_longer(cols = everything(),
               names_to = "variable",
               values_to = "values") %>%
  ggplot(aes(x = values)) +
  geom_histogram() +
  facet_wrap(vars(variable),
             scales = "free")
```

and now from the ACS and ACT data:

```{r expl-cont-acs-act, fig.width=12, fig.height=8}
mn %>%
  # remove the ID variables
  select(-c("GEOID", "District", "District.Nbr")) %>%
  # these variables have at least one lower case character
  select(matches("[a-z]", ignore.case = FALSE)) %>%
  # filter for numerical variables
  select(where(is.numeric)) %>%
  pivot_longer(cols = everything(),
               names_to = "variable",
               values_to = "values") %>%
  ggplot(aes(x = values)) +
  geom_histogram() +
  facet_wrap(vars(variable),
             scales = "free")
```

A lot of the data are right-skewed, which makes sense as many variables are raw counts.

We also checked for missing data:

```{r missing-data}
mn %>% 
  add_n_miss() %>% 
  count(n_miss_all)
```

We had one observation missing 65 variables, and this was the "Remainder of Minnesota" observation from the ACS. The NAs caused issues when we tried to fit a LASSO and random forest model so we removed them beforehand.

```{r}
mn <- mn %>%
  drop_na()
```

## Model Recipe

```{r include=FALSE}
# set the seed
set.seed(21)
```

From our data exploration, these were the things we had to do in the recipe:

* Log-transform most of the variables (except for the ones that start with PCT from the School Survey as they're percentages and PP as they're spending per pupil and not right-skewed)
* Use percentages for ACS variables
* Normalize all numerical variables
* Ignore `PCTTOTA` and `LOCRPAR` as all variables have the same value
* Make the ID variables evaluative (i.e. not included in modeling)

### Split data

We split the data into a training and testing set and also created cross-validation folds for evaluation.

```{r split}
# split the data into a training and test set
mn_split <- initial_split(mn, prop = .75)
mn_training <- training(mn_split)
mn_testing <- testing(mn_split)
```

```{r cv}
# create cross-validation folds
mn_cv <- vfold_cv(mn_training, v = 5)
```

### Preprocess data

Here is the recipe for our models:

```{r recipe}
mn_recipe <- 
  recipe(TotalClasses ~ ., data = mn_training) %>%
  # ignore observations with missing data (necessary for LASSO mod)
  step_naomit(everything(), skip = TRUE) %>%
  # remove variables
  step_rm(
    # this one can be considered as response variable itself
    NumCat, 
    # all variables have the same value for these two
    PCTTOTA, LOCRPAR, 
    # raw counts from ACS
    matches("[a-z]", ignore.case = FALSE),
    -starts_with("perc"),
    -total_pop,
    -Avg_Cmp,
    -TotalClasses
    ) %>%
  # log-transform 
  step_log(
    # total population
    total_pop,
    # spending / revenue variables from the school survey
    ## ignore those that start with P since they're percentages / spending per student
    matches("^[A-OQ-Z]{4,}", ignore.case = FALSE), 
    ## ignore ID variables as well
    -GEOID, -CONUM, -CBSA,
    # some variables have 0s which will produce NaNs when log-transformed
    offset = 1) %>% 
  # make ID variables evaluative (not included in modeling)
  update_role(
    all_of(c("GEOID",
             "District",
             "District.Nbr",
             "CONUM",
             "CSA",
             "CBSA")),
    new_role = "evaluative") %>%
  # make integers numeric
  step_mutate_at(is.integer, fn = as.numeric) %>%
  # normalize numerical variables
  step_normalize(all_predictors())
```

And here is what the data looked like post-transformation:

```{r}
mn_recipe %>%
  prep(mn_training) %>%
  juice()
```

## Model Fitting

### Regular linear regression

We first tested a regular linear regression model and looked at the table of coefficients. Since we had an overwhelming number of predictors, we assumed beforehand that this model would not perform well due to overfitting.

```{r lm}
# define the model type
mn_linear_mod <-
  linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")

# set up the workflow
mn_lm_wf <-
  workflow() %>%
  add_recipe(mn_recipe) %>%
  add_model(mn_linear_mod)

# fit the model
mn_lm_fit <-
  mn_lm_wf %>%
  fit(mn_training)

# display the results
mn_lm_fit %>% 
  pull_workflow_fit() %>% 
  tidy() %>% 
  mutate(across(where(is.numeric), ~round(.x,3))) 
```

### LASSO

To deal with this, we switched to LASSO to shrink coefficients to zero and thereby eliminate insignificant variables from the model.

```{r lasso}
# define the model type
mn_lasso_mod <-
  linear_reg(mixture = 1) %>%
  set_engine("glmnet") %>%
  set_args(penalty = tune()) %>%
  set_mode("regression")

# set up the workflow
mn_lasso_wf <-
  workflow() %>%
  add_recipe(mn_recipe) %>%
  add_model(mn_lasso_mod)

# set up penalty grid for tuning
penalty_grid <- grid_regular(penalty(),
                             levels = 20)

# tune the parameter
mn_lasso_tune <-
  mn_lasso_wf %>%
  tune_grid(
    resamples = mn_cv,
    grid = penalty_grid
  )
```

We chose the best parameter based on RMSE and finalized the workflow / model. Then we looked at the variables that were retained by LASSO.

```{r}
# show the best penalty parameter
mn_lasso_tune %>% 
  show_best(metric = "rmse")

# select best parameter by smallest rmse
(best_param <- mn_lasso_tune %>% 
    select_best(metric = "rmse"))
```

```{r}
# finalize workflow
mn_lasso_final_wf <- mn_lasso_wf %>% 
  finalize_workflow(best_param)

# fit final model
mn_lasso_final_mod <-
  mn_lasso_final_wf %>%
  fit(data = mn_training)

# look at the table of coefficients
mn_lasso_final_mod %>% 
  pull_workflow_fit() %>% 
  tidy()  %>%
  # filter for predictors with non-zero coefficients 
  filter(estimate != 0)
```

### Random forest

Our last candidate was a random forest model.

```{r}
# define the model type
mn_rf_mod <- 
  rand_forest(mtry = 23, # ~1/3 of predictors 
              min_n = 5, 
              trees = 200) %>% 
  set_mode("regression") %>% 
  set_engine("ranger")

# set up the workflow
mn_rf_wf <-
  workflow() %>%
  add_recipe(mn_recipe) %>%
  add_model(mn_rf_mod)

# fit the model
mn_rf_fit <- 
  mn_rf_wf %>% 
  fit(mn_training)
```


## Model evaluation and comparison
 
```{r}
# fit model with best tuning parameter(s) to training data and apply to test data
mn_lm_test <- 
  mn_lm_wf %>% 
  last_fit(mn_split)
mn_lasso_test <- 
  mn_lasso_final_wf %>% 
  last_fit(mn_split)
mn_rf_test <-
  mn_rf_wf %>%
  last_fit(mn_split)
```

Of our three models, the random forest performed best, followed by LASSO and then regular linear regression.

```{r}
# collect metrics for model applied to test data
mn_lm_test %>%
  collect_metrics
mn_lasso_test %>% 
  collect_metrics()
mn_rf_test %>% 
  collect_metrics()
```

### Residuals

Since the RMSE from the regular lm model was rather high, we decided to only compare the LASSO and random forest moving forward. We computed their overall performance metrics and looked at the residuals.

```{r}
lasso_explain <- 
  explain_tidymodels(
    model = mn_lasso_final_mod,
    data = mn_training %>% select(-TotalClasses), 
    y = mn_training %>%  pull(TotalClasses),
    label = "lasso"
  )
```

```{r}
rf_explain <- 
  explain_tidymodels(
    model = mn_rf_fit,
    data = mn_training %>% select(-TotalClasses), 
    y = mn_training %>%  pull(TotalClasses),
    label = "rf"
  )
```

```{r}
# get overall performance metrics
lasso_mod_perf <- model_performance(lasso_explain)
rf_mod_perf <-  model_performance(rf_explain)
```

Here are tables of their performance metrics:

```{r}
# LASSO
data.frame(lasso_mod_perf$measures)
# random forest
data.frame(rf_mod_perf$measures)
```

and the distribution of the residuals:

```{r}
plot(lasso_mod_perf,
     rf_mod_perf, 
     geom = "boxplot")
```

### Variable importance

In the end the random forest showed to greatly outperform the LASSO. Our final step was to look at the variable importance plot from this final model.

```{r}
set.seed(1) 
# create explainer
rf_explain <- 
  explain_tidymodels(
    model = mn_rf_fit,
    data = mn_training %>% select(-TotalClasses), 
    y = mn_training %>%  pull(TotalClasses),
    label = "rf"
  )
# compute variable importance
rf_var_imp <- model_parts(rf_explain)
# plot
plot(rf_var_imp, show_boxplots = TRUE)
```



# Future Work
